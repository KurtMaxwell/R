{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle competition \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in library(dataPreparation): there is no package called 'dataPreparation'\n",
     "output_type": "error",
     "traceback": [
      "Error in library(dataPreparation): there is no package called 'dataPreparation'\nTraceback:\n",
      "1. library(dataPreparation)"
     ]
    }
   ],
   "source": [
    "Sys.setenv(LANG = \"en\")\n",
    "\n",
    "# Data processing library\n",
    "library(data.table)       # Data manipulation\n",
    "library(plyr)             # Data manipulation\n",
    "library(stringr)          # String, text processing             \n",
    "library(dataPreparation)  # Data preparation library\n",
    "library(woeBinning)       # Decision treeâ€“based binning for numerical and categorical variables\n",
    "library(Boruta)           # Variable selection\n",
    "\n",
    "# Machine learning library\n",
    "library(mlr)          # Machine learning framework\n",
    "library(caret)         # Data processing and machine learning framework\n",
    "library(MASS)          # LDA\n",
    "library(randomForest)  # RF\n",
    "library(gbm)           # Boosting Tree\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train <- read.csv('C:/Users/kkusterer/Documents/MBD Semester 2/Kaggle competition/Data & More/bank_mkt_train.csv')\n",
    "# DataSet without Response # \n",
    "test_No_rep<- read.csv('C:/Users/kkusterer/Documents/MBD Semester 2/Kaggle competition/Data & More/bank_mkt_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(test_No_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(full_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the value of campaign\n",
    "full_train[, 'campaign'] <- full_train[, 'campaign'] - 1\n",
    "test_No_rep[, 'campaign'] <- test_No_rep[, 'campaign'] - 1\n",
    "\n",
    "# Checking the value has been reduced\n",
    "min(full_train[, 'campaign'])  # Previously = 1\n",
    "min(test_No_rep[, 'campaign'])  # Previously = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NA's #\n",
    "apply(is.na(full_train), 2, sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split the data into train/validation/test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the full_train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a seed #\n",
    "set.seed(1)\n",
    "\n",
    "# Create a split # \n",
    "train_split <- caret::createDataPartition(y=full_train[, 'subscribe'], p=.7, list=F)\n",
    "train <- full_train[train_split,] # spilt = 70% train\n",
    "valid_test <- full_train[-train_split,] #Spilt = 30% test + vaiidation set\n",
    "\n",
    "valid_split <- caret::createDataPartition(y=valid_test[, 'subscribe'], p=.5, list=F)\n",
    "valid <- valid_test[valid_split,] # 15% - This 50 percent of the 30 percent. \n",
    "test <- valid_test[-valid_split,] # 15%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the subscribe column by number witin each split #\n",
    "table(train$subscribe) \n",
    "table(valid$subscribe) \n",
    "table(test$subscribe) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the VarImp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the most important variables for the random forest #\n",
    "x <- train[,2:(ncol(train)-1)]\n",
    "y <- as.factor(train[,\"subscribe\"])\n",
    "rf_model <- randomForest(x,y, mtry=3, ntree=100, importance=T, seeds=1)\n",
    "pimp_varImp <-  PIMP(x, y, rf_model, S=10, parallel=F, seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which are the most important variables #\n",
    "pimp_varImp$VarImp[order(pimp_varImp$VarImp[, 1], decreasing=T), ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(train$month)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Based on the VAR Imp "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group months into a column for spring "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group months into seasons #\n",
    "train[, 'month_spring'] <- as.logical(train$month %in% c('mar', 'apr', 'may'))\n",
    "valid[, 'month_spring'] <- as.logical(valid$month %in% c('mar', 'apr', 'may'))\n",
    "test[, 'month_spring'] <- as.logical(test$month %in% c('mar', 'apr', 'may'))\n",
    "# For test data\n",
    "test_No_rep[, 'month_spring'] <- as.logical(test_No_rep$month %in% c('mar', 'apr', 'may'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group months into a column for Summer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new variable to train and test (holdout)\n",
    "# Train, valid, test\n",
    "train[, 'month_summer'] <- as.logical(train$month %in% c('jun', 'jul', 'aug'))\n",
    "valid[, 'month_summer'] <- as.logical(valid$month %in% c('jun', 'jul', 'aug'))\n",
    "test[, 'month_summer'] <- as.logical(test$month %in% c('jun', 'jul', 'aug'))\n",
    "# Test (holdout)\n",
    "test_No_rep [, 'month_summer'] <- as.logical(test_No_rep$month %in% c('jun', 'jul', 'aug'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group months into a column for autumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Add new variable to train and test (holdout)\n",
    "# Train, valid, test\n",
    "train[, 'month_autumn'] <- as.logical(train$month %in% c('sep', 'oct', 'nov'))\n",
    "valid[, 'month_autumn'] <- as.logical(valid$month %in% c('sep', 'oct', 'nov'))\n",
    "test[, 'month_autumn'] <- as.logical(test$month %in% c('sep', 'oct', 'nov'))\n",
    "# Test (holdout)\n",
    "test_No_rep[, 'month_autumn'] <- as.logical(test_No_rep$month %in% c('sep', 'oct', 'nov'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group months into a column for winter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new variable to train and test (holdout)\n",
    "# Train, valid, test\n",
    "train[, 'month_winter'] <- as.logical(train$month %in% c('dec', 'jan', 'feb'))\n",
    "valid[, 'month_winter'] <- as.logical(valid$month %in% c('dec', 'jan', 'feb'))\n",
    "test[, 'month_winter'] <- as.logical(test$month %in% c('dec', 'jan', 'feb'))\n",
    "# Test (holdout)\n",
    "test_No_rep[, 'month_winter'] <- as.logical(test_No_rep$month %in% c('dec', 'jan', 'feb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a colmn for the avergae "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new variable to train and test (holdout)\n",
    "# Train, valid, test\n",
    "train[, 'age_ge_mean'] <- as.logical(train$age > mean(train$age))\n",
    "valid[, 'age_ge_mean'] <- as.logical(valid$age > mean(valid$age))\n",
    "test[, 'age_ge_mean'] <- as.logical(test$age > mean(test$age))\n",
    "# Test (holdout)\n",
    "test_No_rep[, 'age_ge_mean'] <- as.logical(test_No_rep$age > mean(train$age))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create column to pdays to 999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new variable to train and test (holdout)\n",
    "# pdays == 999 is a special value\n",
    "# Train, valid, test\n",
    "train[, 'pdays_999'] <- as.logical(train$pdays == 999)\n",
    "valid[, 'pdays_999'] <- as.logical(valid$pdays == 999)\n",
    "test[, 'pdays_999'] <- as.logical(test$pdays == 999)\n",
    "# Test (holdout)\n",
    "test_No_rep[, 'pdays_999'] <- as.logical(test_No_rep$pdays == 999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing the data to be used on the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the variables of the highest importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the IV and DV list name\n",
    "# Dependent variable (DV)\n",
    "dv_list <- c('subscribe')\n",
    "# Independent variable (IV)\n",
    "iv_list <- setdiff(colnames(train), dv_list)  # Exclude the target variable\n",
    "iv_list <- setdiff(iv_list, 'client_id')  # Exclude the client_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick out categorical, boolean and numerical variable\n",
    "# List to be used for categorical variable\n",
    "iv_cat_list <- c()\n",
    "# List to be used for  boolean variable\n",
    "iv_bool_list <- c()\n",
    "# List to be used for numerical variable\n",
    "iv_num_list <- c()  \n",
    "for (v in iv_list) {\n",
    "    if (class(train[, v]) == 'factor') {  # Factor == categorical variable\n",
    "        iv_cat_list <- c(iv_cat_list, v)\n",
    "    } else if (class(train[, v]) == 'logical') {  # Logical == boolean variable\n",
    "        iv_bool_list <- c(iv_bool_list, v)\n",
    "    } else {  # Non-factor + Non-logical == numerical variable\n",
    "        iv_num_list <- c(iv_num_list, v)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binning related variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping 12 categories in the variable job onto 3 groups using WOE\n",
    "binning_cat <- woe.binning(train, 'subscribe', 'job')\n",
    "binning_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the binning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the binning to data\n",
    "tmp <- woe.binning.deploy(train, binning_cat, add.woe.or.dum.var='woe')\n",
    "head(tmp[, c('job', 'job.binned', 'woe.job.binned')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looping throught the cat variables, do this for each split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through all categorical variables\n",
    "for (v in iv_cat_list) {\n",
    "    \n",
    "    # Remapping categorical variable on train data\n",
    "    binning_cat <- woe.binning(train, 'subscribe', v)\n",
    "    \n",
    "    # Apply the binning to the train, valid and test data\n",
    "    train <- woe.binning.deploy(train, binning_cat, add.woe.or.dum.var='woe')\n",
    "    valid <- woe.binning.deploy(valid, binning_cat, add.woe.or.dum.var='woe')\n",
    "    test <- woe.binning.deploy(test, binning_cat, add.woe.or.dum.var='woe')\n",
    "    \n",
    "    # Apply the binning to the test (holdout) data\n",
    "    test_No_rep <- woe.binning.deploy(test_No_rep, binning_cat, add.woe.or.dum.var='woe')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### performing the discetization on a numerical variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping the variable age onto 4 groups using WOE\n",
    "binning_num <- woe.binning(train, 'subscribe', 'age')\n",
    "binning_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the binning to data\n",
    "tmp <- woe.binning.deploy(train, binning_num, add.woe.or.dum.var='woe')\n",
    "head(tmp[, c('age', 'age.binned', 'woe.age.binned')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply to all nummerical variables in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through all numerical variables\n",
    "for (v in iv_num_list) {\n",
    "    \n",
    "    # Discretizing numerical variable on train data\n",
    "    binning_num <- woe.binning(train, 'subscribe', v)\n",
    "    \n",
    "    # Apply the binning to the train, valid and test data\n",
    "    train <- woe.binning.deploy(train, binning_num, add.woe.or.dum.var='woe')\n",
    "    valid <- woe.binning.deploy(valid, binning_num, add.woe.or.dum.var='woe')\n",
    "    test <- woe.binning.deploy(test, binning_num, add.woe.or.dum.var='woe')\n",
    "    \n",
    "    # Apply the binning to the test (holdout) data\n",
    "    test_No_rep <- woe.binning.deploy(test_No_rep, binning_num, add.woe.or.dum.var='woe')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discretization <- focsused on using equal frequency discretization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the discretization\n",
    "bins <- build_bins(dataSet=train, cols=\"age\", n_bins=5, type=\"equal_freq\", verbose=F)\n",
    "\n",
    "# Print out to check\n",
    "bins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### applied to one variale "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to the data\n",
    "tmp <- fastDiscretization(dataSet=train, bins=bins, verbose=F)\n",
    "setDF(tmp); setDF(train)  # Convert data.table to data.frame\n",
    "head(tmp[, 'age'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applied to the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through all numerical variables\n",
    "for (v in iv_num_list) {\n",
    "    \n",
    "    # Discretizing numerical variable on train data, n_bins=5\n",
    "    bins <- build_bins(dataSet=train, cols=v, n_bins=5, type=\"equal_freq\", verbose=F)\n",
    "    \n",
    "    # Apply the binning to the train, valid and test data\n",
    "    tmp <- fastDiscretization(dataSet=train, bins=bins, verbose=F)\n",
    "    setDF(tmp); setDF(train)  # Convert data.table to data.frame\n",
    "    train[, paste0(v, '_freq_bin')] <- tmp[, v]  # Add new variable\n",
    "    \n",
    "    tmp <- fastDiscretization(dataSet=valid, bins=bins, verbose=F)\n",
    "    setDF(tmp); setDF(valid)  # Convert data.table to data.frame\n",
    "    valid[, paste0(v, '_freq_bin')] <- tmp[, v]  # Add new variable\n",
    "    \n",
    "    tmp <- fastDiscretization(dataSet=test, bins=bins, verbose=F)\n",
    "    setDF(tmp); setDF(test)  # Convert data.table to data.frame\n",
    "    test[, paste0(v, '_freq_bin')] <- tmp[, v]  # Add new variable\n",
    "    \n",
    "    # Apply the binning to the test (holdout) data\n",
    "    tmp <- fastDiscretization(dataSet=test_No_rep, bins=bins, verbose=F)\n",
    "    setDF(tmp); setDF(test_No_rep)  # Convert data.table to data.frame\n",
    "    test_No_rep[, paste0(v, '_freq_bin')] <- tmp[, v]  # Add new variable\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build on one variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the discretization\n",
    "bins <- build_bins(dataSet=train, cols=\"age\", n_bins=5, type=\"equal_width\", verbose=F)\n",
    "\n",
    "# Print out to check\n",
    "bins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to the data\n",
    "tmp <- fastDiscretization(dataSet=train, bins=bins, verbose=F)\n",
    "setDF(tmp); setDF(train)  # Convert data.table to data.frame\n",
    "head(tmp[, 'age'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop through all data and apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through all numerical variables\n",
    "for (v in iv_num_list) {\n",
    "    \n",
    "    # Discretizing numerical variable on train data, n_bins=5\n",
    "    bins <- build_bins(dataSet=train, cols=v, n_bins=5, type=\"equal_width\", verbose=F)\n",
    "    \n",
    "    # Apply the binning to the train, valid and test data\n",
    "    tmp <- fastDiscretization(dataSet=train, bins=bins, verbose=F)\n",
    "    setDF(tmp); setDF(train)  # Convert data.table to data.frame\n",
    "    train[, paste0(v, '_width_bin')] <- tmp[, v]  # Add new variable\n",
    "    \n",
    "    tmp <- fastDiscretization(dataSet=valid, bins=bins, verbose=F)\n",
    "    setDF(tmp); setDF(valid)  # Convert data.table to data.frame\n",
    "    valid[, paste0(v, '_width_bin')] <- tmp[, v]  # Add new variable\n",
    "    \n",
    "    tmp <- fastDiscretization(dataSet=test, bins=bins, verbose=F)\n",
    "    setDF(tmp); setDF(test)  # Convert data.table to data.frame\n",
    "    test[, paste0(v, '_width_bin')] <- tmp[, v]  # Add new variable\n",
    "    \n",
    "    # Apply the binning to the test (holdout) data\n",
    "    tmp <- fastDiscretization(dataSet=test_No_rep, bins=bins, verbose=F)\n",
    "    setDF(tmp); setDF(test_No_rep)  # Convert data.table to data.frame\n",
    "    test_No_rep[, paste0(v, '_width_bin')] <- tmp[, v]  # Add new variable\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting updated list of variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the IV and DV list name\n",
    "# Dependent variable (DV)\n",
    "dv_list <- c('subscribe')\n",
    "# Independent variable (IV)\n",
    "iv_list <- setdiff(colnames(train), dv_list)  # Exclude the target variable\n",
    "iv_list <- setdiff(iv_list, 'client_id')  # Exclude the client_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pick out categorical, boolean and numerical variable\n",
    "iv_cat_list <- c()  # List to store categorical variable\n",
    "iv_bool_list <- c()  # List to store boolean variable\n",
    "iv_num_list <- c()  # List to store numerical variable\n",
    "for (v in iv_list) {\n",
    "    if (class(train[, v]) == 'factor') {  # Factor == categorical variable\n",
    "        iv_cat_list <- c(iv_cat_list, v)\n",
    "    } else if (class(train[, v]) == 'logical') {  # Logical == boolean variable\n",
    "        iv_bool_list <- c(iv_bool_list, v)\n",
    "    } else {  # Non-factor + Non-logical == numerical variable\n",
    "        iv_num_list <- c(iv_num_list, v)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy encode a catergorical variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the dummy encoding\n",
    "encoding <- build_encoding(dataSet=train, cols=\"job\", verbose=F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the categorical variable\n",
    "tmp <- one_hot_encoder(dataSet=train, encoding=encoding, type='logical', drop=F, verbose=F)\n",
    "setDF(tmp)\n",
    "tmp <- tmp[, -ncol(tmp)]\n",
    "head(tmp[, 84:ncol(tmp)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USing above method we apply to all the variables in the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through all categorical variables\n",
    "for (v in iv_cat_list) {\n",
    "    \n",
    "    # Representing categorical variable on train data\n",
    "    encoding <- build_encoding(dataSet=train, cols=v, verbose=F)\n",
    "    \n",
    "    # Apply the binning to the train, valid and test data\n",
    "    train <- one_hot_encoder(dataSet=train, encoding=encoding, type='logical', drop=F, verbose=F)\n",
    "    setDF(train)\n",
    "    train <- train[, -ncol(train)]  # Drop the last dummy column\n",
    "    \n",
    "    valid <- one_hot_encoder(dataSet=valid, encoding=encoding, type='logical', drop=F, verbose=F)\n",
    "    setDF(valid)\n",
    "    valid <- valid[, -ncol(valid)]  # Drop the last dummy column\n",
    "    \n",
    "    test <- one_hot_encoder(dataSet=test, encoding=encoding, type='logical', drop=F, verbose=F)\n",
    "    setDF(test)\n",
    "    test <- test[, -ncol(test)]  # Drop the last dummy column\n",
    "    \n",
    "    # Apply the binning to the test (holdout) data\n",
    "    test_No_rep <- one_hot_encoder(dataSet=test_No_rep, encoding=encoding, type='logical', drop=F, verbose=F)\n",
    "    setDF(test_No_rep)\n",
    "    test_No_rep <- test_No_rep[, -ncol(test_No_rep)]  # Drop the last dummy column\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the variable representation on the catergorical variables in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the incidence rates per category of a variable\n",
    "tb <- table(train$job, train$subscribe)\n",
    "incidence_map <- data.frame('v1'=rownames(tb), 'v2'=tb[, '1'] / (tb[, '0'] + tb[, '1']))\n",
    "colnames(incidence_map) <- c('job', 'job_incidence')\n",
    "incidence_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the categories with incidences\n",
    "tmp <- plyr::join(x=train, y=incidence_map, by='job', type=\"left\", match=\"all\")  # Left join\n",
    "head(tmp[, c('job', 'job_incidence')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the above method to the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through all categorical variables\n",
    "for (v in iv_cat_list){\n",
    "    \n",
    "    # Find the incidence rates per category of a variable\n",
    "    tb <- table(train[, v], train[, 'subscribe'])\n",
    "    incidence_map <- data.frame('v1'=rownames(tb), 'v2'=tb[, '1'] / (tb[, '0'] + tb[, '1']))\n",
    "    colnames(incidence_map) <- c(v, paste0(v, '_incidence'))  # Rename the columns to join\n",
    "    \n",
    "    # Apply the variable representation to the train, valid and test data\n",
    "    train <- plyr::join(x=train, y=incidence_map, by=v, type=\"left\", match=\"all\")\n",
    "    valid <- plyr::join(x=valid, y=incidence_map, by=v, type=\"left\", match=\"all\")\n",
    "    test <- plyr::join(x=test, y=incidence_map, by=v, type=\"left\", match=\"all\")\n",
    "    \n",
    "    # Apply the binning to the test (holdout) data\n",
    "    test_No_rep <- plyr::join(x=test_No_rep, y=incidence_map, by=v, type=\"left\", match=\"all\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the variable representation on the catergorical variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the WOE per category of a variable\n",
    "tb <- table(train$job, train$subscribe)\n",
    "woe_map <- data.frame('v1'=rownames(tb), 'v2'=log(tb[, '1'] / tb[, '0']))\n",
    "colnames(woe_map) <- c('job', 'job_woe')\n",
    "woe_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the categories with WOE\n",
    "tmp <- plyr::join(x=train, y=woe_map, by='job', type=\"left\", match=\"all\")  # Left join\n",
    "head(tmp[, c('job', 'job_woe')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying above method to the all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through all categorical variables\n",
    "for (v in iv_cat_list) {\n",
    "    \n",
    "    # Find the incidence rates per category of a variable\n",
    "    tb <- table(train[, v], train[, 'subscribe'])\n",
    "    woe_map <- data.frame('v1'=rownames(tb), 'v2'=log(tb[, '1'] / tb[, '0']))\n",
    "    colnames(woe_map) <- c(v, paste0(v, '_woe'))  # Rename the columns to join\n",
    "    \n",
    "    # Apply the variable representation to the train, valid and test data\n",
    "    train <- plyr::join(x=train, y=woe_map, by=v, type=\"left\", match=\"all\")\n",
    "    valid <- plyr::join(x=valid, y=woe_map, by=v, type=\"left\", match=\"all\")\n",
    "    test <- plyr::join(x=test, y=woe_map, by=v, type=\"left\", match=\"all\")\n",
    "    \n",
    "    # Apply the binning to the test (Test_No_Rep) data\n",
    "    test_No_rep <- plyr::join(x=test_No_rep, y=woe_map, by=v, type=\"left\", match=\"all\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take the log of Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the variable age on train and test (Test_No_rep)\n",
    "# Train, valid, test\n",
    "train[, 'age_log'] <- log(train[, 'age'])\n",
    "valid[, 'age_log'] <- log(valid[, 'age'])\n",
    "test[, 'age_log'] <- log(test[, 'age'])\n",
    "# Test (Test_No_Rep)\n",
    "test_No_rep[, 'age_log'] <- log(test_No_rep[, 'age'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then to standardize the numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the variable age on train and test (Test_No_rep)\n",
    "# Train, valid, test\n",
    "train[, 'age_scaled'] <- scale(train[, 'age'], center=T, scale=T)  # sd = 1, mean = 0\n",
    "valid[, 'age_scaled'] <- scale(valid[, 'age'], center=T, scale=T)  # sd = 1, mean = 0\n",
    "test[, 'age_scaled'] <- scale(test[, 'age'], center=T, scale=T)  # sd = 1, mean = 0\n",
    "# Test (Test_No_rep)\n",
    "test_No_rep[, 'age_scaled'] <- scale(test_No_rep[, 'age'], center=T, scale=T)  # sd = 1, mean = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Selection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating the list catorgoriacl variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the IV and DV list name\n",
    "# Dependent variable (DV)\n",
    "dv_list <- c('subscribe')\n",
    "# Independent variable (IV)\n",
    "iv_list <- setdiff(colnames(train), dv_list)  # Exclude the target variable\n",
    "iv_list <- setdiff(iv_list, 'client_id')  # Exclude the client_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick out categorical, boolean and numerical variable\n",
    "iv_cat_list <- c()  # List to store categorical variable\n",
    "iv_bool_list <- c()  # List to store boolean variable\n",
    "iv_num_list <- c()  # List to store numerical variable\n",
    "for (v in iv_list) {\n",
    "    if (class(train[, v]) == 'factor') {  # Factor == categorical variable\n",
    "        iv_cat_list <- c(iv_cat_list, v)\n",
    "    } else if (class(train[, v]) == 'logical') {  # Logical == boolean variable\n",
    "        iv_bool_list <- c(iv_bool_list, v)\n",
    "    } else {  # Non-factor + Non-logical == numerical variable\n",
    "        iv_num_list <- c(iv_num_list, v)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check values for inf +- -> potential outliers in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing value\n",
    "# Train, valid, test\n",
    "sum(apply(sapply(train, is.infinite), 2, sum))\n",
    "sum(apply(sapply(valid, is.infinite), 2, sum))\n",
    "sum(apply(sapply(test, is.infinite), 2, sum))\n",
    "# Test (holdout)\n",
    "sum(apply(sapply(test_No_rep, is.infinite), 2, sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute +/-Inf value by NA\n",
    "# Train, valid, test\n",
    "train[sapply(train, is.infinite)] <- NA\n",
    "valid[sapply(valid, is.infinite)] <- NA\n",
    "test[sapply(test, is.infinite)] <- NA\n",
    "# Test (holdout)\n",
    "test_No_rep[sapply(test_No_rep, is.infinite)] <- NA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check and correct any potential NA values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing value\n",
    "# Train, valid, test\n",
    "sum(apply(is.na(train), 2, sum))\n",
    "sum(apply(is.na(valid), 2, sum))\n",
    "sum(apply(is.na(test), 2, sum))\n",
    "# Test (holdout)\n",
    "sum(apply(is.na(test_No_rep), 2, sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing value in numerical variable by mean\n",
    "for (v in iv_num_list) {\n",
    "    # Train, valid, test\n",
    "    train[is.na(train[, v]), v] <- mean(train[, v], na.rm=T)\n",
    "    valid[is.na(valid[, v]), v] <- mean(valid[, v], na.rm=T)\n",
    "    test[is.na(test[, v]), v] <- mean(test[, v], na.rm=T)\n",
    "    \n",
    "    # Test (holdout)\n",
    "    test_No_rep[is.na(test_No_rep[, v]), v] <- mean(test_No_rep[, v], na.rm=T)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping catergorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (v in iv_cat_list) {\n",
    "    # Train, valid, test\n",
    "    train[, v] <- NULL\n",
    "    valid[, v] <- NULL\n",
    "    test[, v] <- NULL\n",
    "    \n",
    "    # Test (Test_No_Rep)\n",
    "    test_No_rep[, v] <- NULL\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concerting Boolean Varaibels to integer variabels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert boolean to int\n",
    "for (v in iv_bool_list) {\n",
    "    # Train, valid, test\n",
    "    train[, v] <- as.integer(train[, v])\n",
    "    valid[, v] <- as.integer(valid[, v])\n",
    "    test[, v] <- as.integer(test[, v])\n",
    "    \n",
    "    # Test (Test_No_Rep)\n",
    "    test_No_rep[, v] <- as.integer(test_No_rep[, v])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find any constant Varaibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the constant variable\n",
    "var_list <- c()\n",
    "for (v in c(iv_num_list, iv_bool_list)) {\n",
    "    var_list <- c(var_list, var(train[, v], na.rm=T))\n",
    "}\n",
    "constant_var <- c(iv_num_list, iv_bool_list)[var_list == 0]\n",
    "constant_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop the constant Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the constant variable\n",
    "for (v in constant_var) {\n",
    "    # Train, valid, test\n",
    "    train[, v] <- NULL\n",
    "    valid[, v] <- NULL\n",
    "    test[, v] <- NULL\n",
    "    \n",
    "    # Test (Test_No_rep)\n",
    "    test_No_rep[, v] <- NULL\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the fishcer score, for variable importance as a predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FisherScore <- function(basetable, depvar, IV_list) {\n",
    "  \"\n",
    "  This function calculate the Fisher score of a variable.\n",
    "  \n",
    "  Ref:\n",
    "  ---\n",
    "  Verbeke, W., Dejaeger, K., Martens, D., Hur, J., & Baesens, B. (2012). New insights into churn prediction in the telecommunication sector: A profit driven data mining approach. European Journal of Operational Research, 218(1), 211-229.\n",
    "  \"\n",
    "  \n",
    "  # Get the unique values of dependent variable\n",
    "  DV <- unique(basetable[, depvar])\n",
    "  \n",
    "  IV_FisherScore <- c()\n",
    "  \n",
    "  for (v in IV_list) {\n",
    "    fs <- abs((mean(basetable[which(basetable[, depvar]==DV[1]), v]) - mean(basetable[which(basetable[, depvar]==DV[2]), v]))) /\n",
    "      sqrt((var(basetable[which(basetable[, depvar]==DV[1]), v]) + var(basetable[which(basetable[, depvar]==DV[2]), v])))\n",
    "    IV_FisherScore <- c(IV_FisherScore, fs)\n",
    "  }\n",
    "  \n",
    "  return(data.frame(IV=IV_list, fisher_score=IV_FisherScore))\n",
    "}\n",
    "\n",
    "varSelectionFisher <- function(basetable, depvar, IV_list, num_select=20) {\n",
    "  \"\n",
    "  This function will calculate the Fisher score for all IVs and select the best\n",
    "  top IVs.\n",
    "\n",
    "  Assumption: all variables of input dataset are converted into numeric type.\n",
    "  \"\n",
    "  \n",
    "  fs <- FisherScore(basetable, depvar, IV_list)  # Calculate Fisher Score for all IVs\n",
    "  num_select <- min(num_select, ncol(basetable))  # Top N IVs to be selected\n",
    "  return(as.vector(fs[order(fs$fisher_score, decreasing=T), ][1:num_select, 'IV']))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Fisher Score for all variable\n",
    "# Get the IV and DV list\n",
    "dv_list <- c('subscribe')  # DV list\n",
    "iv_list <- setdiff(names(train), dv_list)  # IV list excluded DV\n",
    "iv_list <- setdiff(iv_list, 'client_id')  # Excluded the client_id\n",
    "fs <- FisherScore(train, dv_list, iv_list)\n",
    "head(fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top 20 variables according to the Fisher Score\n",
    "best_fs_var <- varSelectionFisher(train, dv_list, iv_list, num_select=50)\n",
    "head(best_fs_var, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply variable selection to the data\n",
    "# Train\n",
    "var_select <- names(train)[names(train) %in% best_fs_var]\n",
    "train_processed <- train[, c('client_id', var_select, 'subscribe')]\n",
    "# Valid\n",
    "var_select <- names(valid)[names(valid) %in% best_fs_var]\n",
    "valid_processed <- valid[, c('client_id', var_select, 'subscribe')]\n",
    "# Test\n",
    "var_select <- names(test)[names(test) %in% best_fs_var]\n",
    "test_processed <- test[, c('client_id', var_select, 'subscribe')]\n",
    "# Test (holdout)\n",
    "var_select <- names(test_No_rep)[names(test_No_rep) %in% best_fs_var]\n",
    "test_No_rep_processed <- test_No_rep[, c('client_id', var_select)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final data preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if train and test (holdout) have same variables\n",
    "# Train, valid, test\n",
    "dim(train_processed)\n",
    "dim(valid_processed)\n",
    "dim(test_processed)\n",
    "# Test (holdout)\n",
    "dim(test_No_rep_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the data columns\n",
    "for (v in colnames(train_processed)) {\n",
    "    \n",
    "    # Fix the column name\n",
    "    fix_name <- str_replace_all(v, \"[^[:alnum:] ]\", \"_\")\n",
    "    fix_name <- gsub(' +', '', fix_name) \n",
    "    \n",
    "    # Train, valid, test\n",
    "    colnames(train_processed)[colnames(train_processed) == v] <- fix_name\n",
    "    colnames(valid_processed)[colnames(valid_processed) == v] <- fix_name\n",
    "    colnames(test_processed)[colnames(test_processed) == v] <- fix_name\n",
    "    \n",
    "    # Test (holdout)\n",
    "    colnames(test_No_rep_processed)[colnames(test_No_rep_processed) == v] <- fix_name\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rename Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pro <- train_processed\n",
    "valid_pro <- valid_processed\n",
    "test_pro <- test_processed\n",
    "testNrep <- test_No_rep_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up cross-validation\n",
    "rdesc = makeResampleDesc(\"CV\", iters=5, predict=\"both\")\n",
    "\n",
    "# Define the model\n",
    "learner <- makeLearner(\"classif.logreg\", predict.type=\"prob\", fix.factors.prediction=T)\n",
    "\n",
    "# Define the task\n",
    "train_task <- makeClassifTask(id=\"bank_train\", data=train_pro[, -1], target=\"subscribe\")\n",
    "\n",
    "# Set hyper parameter tuning\n",
    "tune_params <- makeParamSet(\n",
    ")\n",
    "ctrl = makeTuneControlGrid()\n",
    "\n",
    "# Run the hyper parameter tuning with k-fold CV\n",
    "if (length(tune_params$pars) > 0) {\n",
    "    # Run parameter tuning\n",
    "    res <- tuneParams(learner, task=train_task, resampling=rdesc,\n",
    "      par.set=tune_params, control=ctrl, measures=list(mlr::auc))\n",
    "    \n",
    "    # Extract best model\n",
    "    best_learner <- res$learner\n",
    "    \n",
    "} else {\n",
    "    # Simple cross-validation\n",
    "    res <- resample(learner, train_task, rdesc, measures=list(mlr::auc, setAggregation(mlr::auc, train.mean)))\n",
    "    \n",
    "    # No parameter for tuning, only 1 best learner\n",
    "    best_learner <- learner\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the model with tbe best hyper-parameters\n",
    "best_md <- mlr::train(best_learner, train_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction on valid data\n",
    "pred_log <- predict(best_md, newdata=valid_processed[, -1])\n",
    "#performance(pred_log, measures=mlr::auc) <-error here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up cross-validation\n",
    "rdesc = makeResampleDesc(\"CV\", iters=10)\n",
    "\n",
    "# Define the model\n",
    "learner <- makeLearner(\"classif.randomForest\", predict.type=\"prob\", fix.factors.prediction=T)\n",
    "\n",
    "# Define the task\n",
    "train_task <- makeClassifTask(id=\"bank_train\", data=train_pro[, -1], target=\"subscribe\")\n",
    "\n",
    "# Set hyper parameter tuning\n",
    "tune_params <- makeParamSet(\n",
    "  makeDiscreteParam('ntree', value=c(100, 250, 500, 750, 1000)),\n",
    "  makeDiscreteParam('mtry', value=round(sqrt((ncol(train_processed)-1) * c(0.1, 0.25, 0.5, 1, 2, 4))))\n",
    ")\n",
    "ctrl = makeTuneControlGrid()\n",
    "\n",
    "# Run the hyper parameter tuning with k-fold CV\n",
    "if (length(tune_params$pars) > 0) {\n",
    "    # Run parameter tuning\n",
    "    res <- tuneParams(learner, task=train_task, resampling=rdesc,\n",
    "      par.set=tune_params, control=ctrl, measures=list(mlr::auc))\n",
    "    \n",
    "    # Extract best model\n",
    "    best_learner <- res$learner\n",
    "    \n",
    "} else {\n",
    "    # Simple cross-validation\n",
    "    res <- resample(learner, train_task, rdesc, measures=list(mlr::auc))\n",
    "    \n",
    "    # No parameter for tuning, only 1 best learner\n",
    "    best_learner <- learner\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the model with tbe best hyper-parameters\n",
    "best_md <- mlr::train(best_learner, train_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction on valid data\n",
    "pred <- predict(best_md, newdata=valid_pro[, -1])\n",
    "#performance(pred, measures=mlr::auc) <- error here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class(pred) # pred of type prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction on test data\n",
    "pred <- predict(best_md, newdata=test_pro[, -1])\n",
    "performance(pred, measures=mlr::auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction on test data\n",
    "pred <- predict(best_md, newdata=testNrep[, -1])\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K nearest neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can use the class package in using the KNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install.packages(\"class\")\n",
    "library(class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncol(train_pro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Normilization \n",
    "\n",
    "normalize <- function(x) {\n",
    "  return ((x - min(x)) / (max(x) - min(x))) }\n",
    "\n",
    "norm_knn <- as.data.frame(lapply(train_pro[,1:51], normalize))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(123)\n",
    "\n",
    "data_knn <- sample(1:nrow(norm_knn),size=nrow(norm_knn)*0.7,replace = FALSE) \n",
    "\n",
    "#Creation of labels\n",
    "train_knn <- train_pro[,52]\n",
    "test_knn <- test_pro[,52]\n",
    "\n",
    "# Running KNN on the test\n",
    "knn_pred_test <- kNN(train = train_pro, test=test_pro, cl=train_knn, k = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install relevant package to produce output\n",
    "install.pacakges(gmodels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(gmodels)\n",
    "# Create visualisation for KNN\n",
    "Crosstable <- (x = test_knn, y = knn_pred_test, prop.chisp=FALSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model_Accuracy = (TN+TP/Total_Observations) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    " ## Gardient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(123)\n",
    "\n",
    "#Training the model \n",
    "\n",
    "subscribe_model <-gbm(formula = subscribe ~ ., \n",
    "                    distribution = \"bernoulli\", \n",
    "                    data = train_pro,\n",
    "                    n.trees = 10000)\n",
    "# Print the model\n",
    "\n",
    "print(subscribe_model)\n",
    "\n",
    "#summary() the variable importance\n",
    "\n",
    "summary(subscribe_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating predictions with the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gerneraton on test\n",
    "preds1 <- predict(object = subscribe_model,\n",
    "                     newdata = test_pro,\n",
    "                     n.trees = 10000)\n",
    "#generation on test (scaled)\n",
    "preds2 <- predict(object = subscribe_model,\n",
    "                     newdata = test_pro,\n",
    "                     n.trees = \"response\")\n",
    "\n",
    "#Comparing the reange of the 2 predictions \n",
    "range(preds1)\n",
    "range(preds2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the test Auc's for both sets of predications and compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc(actual = test_pro$subscribe, predicted = preds1)#default \n",
    "auc(actual = test_pro$subscribe, predicted = preds2)#rescaled <- error in auc function missing a required argument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative Method using/ mlbench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Install.packages(\"mlbench\")\n",
    "library(mlbench)\n",
    "\n",
    "# Make a task\n",
    "regr.task = makeRegrTask(data = train_pro, target = \"subsrcibe\")\n",
    "regr.task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(1234)\n",
    "\n",
    "# Define a search space for each learner'S parameter\n",
    "ps_ksvm = makeParamSet(\n",
    "  makeNumericParam(\"sigma\", lower = -12, upper = 12, trafo = function(x) 2^x)\n",
    ")\n",
    "\n",
    "ps_rf = makeParamSet(\n",
    "  makeIntegerParam(\"num.trees\", lower = 1L, upper = 200L)\n",
    ")\n",
    "\n",
    "# Choose a resampling strategy\n",
    "rdesc = makeResampleDesc(\"CV\", iters = 5L)\n",
    "\n",
    "# Choose a performance measure\n",
    "meas = rmse\n",
    "\n",
    "# Choose a tuning method\n",
    "ctrl = makeTuneControlCMAES(budget = 100L)\n",
    "\n",
    "# Make tuning wrappers\n",
    "tuned.ksvm = makeTuneWrapper(learner = \"regr.ksvm\", resampling = rdesc, measures = meas,\n",
    "  par.set = ps_ksvm, control = ctrl, show.info = FALSE)\n",
    "tuned.rf = makeTuneWrapper(learner = \"regr.ranger\", resampling = rdesc, measures = meas,\n",
    "  par.set = ps_rf, control = ctrl, show.info = FALSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Four learners to be compared\n",
    "lrns = list(makeLearner(\"regr.lm\"), tuned.ksvm, tuned.rf)\n",
    "\n",
    "# Conduct the benchmark experiment\n",
    "bmr = benchmark(learners = lrns, tasks = regr.task, resamplings = rdesc, measures = rmse, \n",
    "  show.info = FALSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get performances \n",
    "getBMRAggrPerformances(bmr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Method for linear Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(readxl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On training \n",
    "lmSubscribe = lm(subscribe ~., data=train_pro)\n",
    "summary(lmSubscribe)\n",
    "# On validation \n",
    "lmSubscribe = lm(subscribe ~., data=valid_pro)\n",
    "summary(lmSubscribe)\n",
    "\n",
    "#On test\n",
    "lmSubscribe = lm(subscribe ~., data=test_pro)\n",
    "summary(lmSubscribe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The above shows which features are good for predicting subscribe and which features are not "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruuning a linear regression with some more related features \n",
    "\n",
    "lm_subscribe2 <- lm(subscribe ~ pdays + woe_cons_conf_idx_binned + woe_euribor3m_binned, data = train_pro)\n",
    "summary(lm_subscribe2)\n",
    "plot(lm_subscibe2, pch = 16, color =\"red\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
